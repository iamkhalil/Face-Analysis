{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e884004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55728bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from disk (data folder) and rescale them\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5715bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess all the images\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/test',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f25e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 19:28:32.597951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-15 19:28:34.288589: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-11-15 19:28:34.288627: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2021-11-15 19:28:34.289403: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-11-15 19:28:34.430824: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1995400000 Hz\n",
      "2021-11-15 19:28:34.431937: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5a040013d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-15 19:28:34.431976: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "# Implement a CNN to tackle the FER-2013 dataset\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, 3, activation=\"relu\", input_shape=[48, 48, 1]), # images are 28 Ã— 28 pixels, with a single color (grayscale) \n",
    "    keras.layers.Conv2D(64, 3, activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(2), # divide each spatial dimension by a factor of 2\n",
    "    keras.layers.Dropout(0.25), \n",
    "    \n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    \n",
    "    # Next is the fully connected network, composed of a hidden dense layer and a dense output layer.\n",
    "    keras.layers.Flatten(), # convert each input image into a 1D array\n",
    "    keras.layers.Dense(1024, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5), # dropout rate of 50% to reduce overfitting\n",
    "    keras.layers.Dense(7, activation=\"softmax\"), # Dense output layer with 7 neurons (one per class) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999ccf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 44, 44, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 2,345,607\n",
      "Trainable params: 2,345,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display all the model's layers \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328e2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(lr=0.0001, decay=1e-6),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a957fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 122s 272ms/step - loss: 1.8007 - accuracy: 0.2625 - val_loss: 1.6969 - val_accuracy: 0.3527\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 128s 285ms/step - loss: 1.6198 - accuracy: 0.3683 - val_loss: 1.5265 - val_accuracy: 0.4205\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 127s 284ms/step - loss: 1.5104 - accuracy: 0.4197 - val_loss: 1.4403 - val_accuracy: 0.4482\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 127s 284ms/step - loss: 1.4370 - accuracy: 0.4516 - val_loss: 1.3748 - val_accuracy: 0.4754\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 125s 279ms/step - loss: 1.3738 - accuracy: 0.4763 - val_loss: 1.3216 - val_accuracy: 0.4933\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 127s 283ms/step - loss: 1.3231 - accuracy: 0.4987 - val_loss: 1.2953 - val_accuracy: 0.5093\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 128s 287ms/step - loss: 1.2755 - accuracy: 0.5178 - val_loss: 1.2471 - val_accuracy: 0.5255\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 128s 285ms/step - loss: 1.2414 - accuracy: 0.5328 - val_loss: 1.2243 - val_accuracy: 0.5317\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 127s 283ms/step - loss: 1.2033 - accuracy: 0.5492 - val_loss: 1.2038 - val_accuracy: 0.5374\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 125s 279ms/step - loss: 1.1762 - accuracy: 0.5557 - val_loss: 1.1820 - val_accuracy: 0.5488\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 126s 281ms/step - loss: 1.1509 - accuracy: 0.5676 - val_loss: 1.1880 - val_accuracy: 0.5497\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 125s 280ms/step - loss: 1.1223 - accuracy: 0.5779 - val_loss: 1.1549 - val_accuracy: 0.5562\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 125s 280ms/step - loss: 1.0988 - accuracy: 0.5898 - val_loss: 1.1363 - val_accuracy: 0.5688\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 126s 282ms/step - loss: 1.0693 - accuracy: 0.5990 - val_loss: 1.1404 - val_accuracy: 0.5628\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 127s 284ms/step - loss: 1.0517 - accuracy: 0.6047 - val_loss: 1.1164 - val_accuracy: 0.5777\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 129s 288ms/step - loss: 1.0287 - accuracy: 0.6147 - val_loss: 1.1110 - val_accuracy: 0.5762\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 134s 300ms/step - loss: 1.0040 - accuracy: 0.6285 - val_loss: 1.1167 - val_accuracy: 0.5795\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 125s 279ms/step - loss: 0.9787 - accuracy: 0.6353 - val_loss: 1.1009 - val_accuracy: 0.5829\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 126s 282ms/step - loss: 0.9548 - accuracy: 0.6457 - val_loss: 1.0880 - val_accuracy: 0.5957\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 125s 280ms/step - loss: 0.9321 - accuracy: 0.6557 - val_loss: 1.0792 - val_accuracy: 0.5922\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 127s 283ms/step - loss: 0.9099 - accuracy: 0.6642 - val_loss: 1.0955 - val_accuracy: 0.5929\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 128s 286ms/step - loss: 0.8873 - accuracy: 0.6686 - val_loss: 1.0833 - val_accuracy: 0.5917\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 129s 288ms/step - loss: 0.8605 - accuracy: 0.6831 - val_loss: 1.0852 - val_accuracy: 0.5970\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 137s 307ms/step - loss: 0.8372 - accuracy: 0.6922 - val_loss: 1.0798 - val_accuracy: 0.5995\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 125s 279ms/step - loss: 0.8162 - accuracy: 0.6967 - val_loss: 1.0714 - val_accuracy: 0.6046\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 126s 282ms/step - loss: 0.7920 - accuracy: 0.7115 - val_loss: 1.0756 - val_accuracy: 0.6071\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 128s 286ms/step - loss: 0.7707 - accuracy: 0.7160 - val_loss: 1.0892 - val_accuracy: 0.6063\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 129s 288ms/step - loss: 0.7510 - accuracy: 0.7276 - val_loss: 1.1018 - val_accuracy: 0.6117\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 140s 313ms/step - loss: 0.7268 - accuracy: 0.7335 - val_loss: 1.0757 - val_accuracy: 0.6102\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 140s 313ms/step - loss: 0.7037 - accuracy: 0.7421 - val_loss: 1.0767 - val_accuracy: 0.6091\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 140s 312ms/step - loss: 0.6847 - accuracy: 0.7498 - val_loss: 1.0885 - val_accuracy: 0.6138\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 138s 308ms/step - loss: 0.6597 - accuracy: 0.7622 - val_loss: 1.0848 - val_accuracy: 0.6150\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 138s 308ms/step - loss: 0.6391 - accuracy: 0.7673 - val_loss: 1.0937 - val_accuracy: 0.6168\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 138s 308ms/step - loss: 0.6192 - accuracy: 0.7743 - val_loss: 1.1038 - val_accuracy: 0.6148\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 137s 306ms/step - loss: 0.5923 - accuracy: 0.7841 - val_loss: 1.1129 - val_accuracy: 0.6143\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 127s 283ms/step - loss: 0.5833 - accuracy: 0.7900 - val_loss: 1.1110 - val_accuracy: 0.6210\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 127s 282ms/step - loss: 0.5667 - accuracy: 0.7938 - val_loss: 1.1127 - val_accuracy: 0.6223\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 126s 282ms/step - loss: 0.5369 - accuracy: 0.8074 - val_loss: 1.1293 - val_accuracy: 0.6207\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 126s 281ms/step - loss: 0.5227 - accuracy: 0.8128 - val_loss: 1.1204 - val_accuracy: 0.6200\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 127s 284ms/step - loss: 0.5050 - accuracy: 0.8166 - val_loss: 1.1326 - val_accuracy: 0.6222\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 127s 284ms/step - loss: 0.4865 - accuracy: 0.8227 - val_loss: 1.1789 - val_accuracy: 0.6247\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 129s 288ms/step - loss: 0.4675 - accuracy: 0.8315 - val_loss: 1.1497 - val_accuracy: 0.6244\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 130s 290ms/step - loss: 0.4609 - accuracy: 0.8330 - val_loss: 1.1616 - val_accuracy: 0.6214\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 128s 287ms/step - loss: 0.4426 - accuracy: 0.8390 - val_loss: 1.1691 - val_accuracy: 0.6251\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 126s 282ms/step - loss: 0.4210 - accuracy: 0.8484 - val_loss: 1.1978 - val_accuracy: 0.6278\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 127s 284ms/step - loss: 0.4120 - accuracy: 0.8514 - val_loss: 1.1686 - val_accuracy: 0.6286\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 127s 283ms/step - loss: 0.3979 - accuracy: 0.8544 - val_loss: 1.1943 - val_accuracy: 0.6272\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 128s 286ms/step - loss: 0.3793 - accuracy: 0.8627 - val_loss: 1.2234 - val_accuracy: 0.6282\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 130s 289ms/step - loss: 0.3759 - accuracy: 0.8653 - val_loss: 1.2047 - val_accuracy: 0.6282\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 130s 289ms/step - loss: 0.3575 - accuracy: 0.8713 - val_loss: 1.2377 - val_accuracy: 0.6219\n"
     ]
    }
   ],
   "source": [
    "# train the model \n",
    "history = model.fit(train_generator, steps_per_epoch=28709 // 64,\n",
    "                    epochs=50, validation_data=validation_generator,\n",
    "                    validation_steps=7178 // 64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2c4ffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17917/3544183496.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set the vertical range to [0-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e5ac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 8s 68ms/step - loss: 1.9397 - accuracy: 0.2473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.939704179763794, 0.24728336930274963]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model \n",
    "model.evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c50b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modelâ€™s architecture & the values of all the model parameters for every layer (since we're using the Sequential API)\n",
    "model.save(\"emotion_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "422628ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                ########################\n",
    "                                # Face detection phase #\n",
    "                                ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40d6fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7add0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-21 13:12:35.496519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-21 13:12:36.830709: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-11-21 13:12:36.830763: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2021-11-21 13:12:36.831400: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-11-21 13:12:36.857239: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1995460000 Hz\n",
      "2021-11-21 13:12:36.858438: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe380000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-21 13:12:36.858489: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "# load the model \n",
    "model = keras.models.load_model(\"emotion_model.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40765374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haar cascade based detector \n",
    "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# start the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image\n",
    "    if not ret:\n",
    "        continue\n",
    "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
    "    for (x,y,w,h) in faces_detected:\n",
    "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7) \n",
    "        roi_gray=gray_img[y:y+w,x:x+h]  #cropping region of interest i.e. face area from  image\n",
    "        roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "        img_pixels = image.img_to_array(roi_gray)\n",
    "        img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "        img_pixels /= 255\n",
    "        \n",
    "        predictions = model.predict(img_pixels)\n",
    "\n",
    "        #find max indexed array\n",
    "        max_index = np.argmax(predictions[0])\n",
    "\n",
    "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "        predicted_emotion = emotions[max_index]\n",
    "\n",
    "        # Age prediction\n",
    "        MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "        age_list = ['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)', '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)']\n",
    "        \n",
    "        age_net = cv2.dnn.readNetFromCaffe('deploy_age.prototxt', 'age_net.caffemodel') \n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(test_img, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        age_net.setInput(blob)\n",
    "        age_preds = age_net.forward()\n",
    "        predicted_age = age_list[age_preds[0].argmax()]\n",
    "        \n",
    "        label = \"{} | {}\".format(predicted_emotion, predicted_age)\n",
    "        cv2.putText(test_img, label, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "\n",
    "    resized_img = cv2.resize(test_img, (1000, 700))\n",
    "    cv2.imshow('Facial emotion/age analysis', resized_img)\n",
    "    \n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "211c72c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 11:19:30.725066: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2021-11-20 11:19:30.725194: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2021-11-20 11:19:30.727050: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\n",
      "2021-11-20 11:19:30.727083: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "2021-11-20 11:19:30.727088: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "2021-11-20 11:19:30.853554: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2021-11-20 11:19:30.853706: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2021-11-20 11:19:30.898459: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\n",
      "2021-11-20 11:19:30.898501: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 40 nodes (-12), 39 edges (-12), time = 23.385ms.\n",
      "2021-11-20 11:19:30.898508: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 40 nodes (0), 39 edges (0), time = 5.279ms.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the deployment to the RPI3\n",
    "# https://www.tensorflow.org/lite/convert\n",
    "\n",
    "# Convert the model \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open('emotion_model_no_opt.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfc8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
